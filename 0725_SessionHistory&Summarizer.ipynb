{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessiechd/RAG_Model/blob/main/0725_SessionHistory%26Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load Tinyllama"
      ],
      "metadata": {
        "id": "2fp56lyvld1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# === Load TinyLlama ===\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# === Inference Wrapper ===\n",
        "def run_tiny_llama(prompt, max_new_tokens=256):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def build_chunk_prompt(mini_chunks):\n",
        "    annotated = \"\\n\".join([f\"<CHUNK{i}> {text}\" for i, text in enumerate(mini_chunks)])\n",
        "    instructions = (\n",
        "        \"You are a smart text segmenter. Group the annotated mini-chunks into larger, semantically coherent chunks.\\n\"\n",
        "        \"Each chunk should combine 2-4 mini-chunks that belong together in meaning.\\n\"\n",
        "        \"Respond with groups using this format:\\n\\n\"\n",
        "        \"Chunk 1: <CHUNK0>, <CHUNK1>\\n\"\n",
        "        \"Chunk 2: <CHUNK2>, <CHUNK3>, <CHUNK4>\\n\"\n",
        "    )\n",
        "    return f\"{instructions}\\n\\n{annotated}\"\n",
        "\n",
        "def agentic_chunk_text(text, section_title, max_chars=300):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    mini_chunks, temp = [], \"\"\n",
        "    for sent in sentences:\n",
        "        if len(temp) + len(sent) < max_chars:\n",
        "            temp += \" \" + sent\n",
        "        else:\n",
        "            mini_chunks.append(temp.strip())\n",
        "            temp = sent\n",
        "    if temp:\n",
        "        mini_chunks.append(temp.strip())\n",
        "\n",
        "    if len(mini_chunks) <= 1:\n",
        "        return [f\"## {section_title}\\n\" + text.strip()], [\"Kept as one chunk\"]\n",
        "\n",
        "    if len(mini_chunks) > 12:\n",
        "        mini_chunks = mini_chunks[:12]\n",
        "\n",
        "    prompt = build_chunk_prompt(mini_chunks)\n",
        "    raw_output = run_tiny_llama(prompt)\n",
        "\n",
        "    grouped_chunks = []\n",
        "    explanation = []\n",
        "    for line in raw_output.splitlines():\n",
        "        if line.startswith(\"Chunk\"):\n",
        "            refs = re.findall(r\"<CHUNK(\\d+)>\", line)\n",
        "            if refs:\n",
        "                valid_refs = [int(i) for i in refs if int(i) < len(mini_chunks)]\n",
        "                if not valid_refs:\n",
        "                    continue\n",
        "                group_text = \" \".join([mini_chunks[i] for i in valid_refs])\n",
        "                cleaned_text = re.sub(r\"\\s+\", \" \", group_text.strip())\n",
        "                grouped_chunks.append(f\"## {section_title}\\n\" + cleaned_text)\n",
        "                if len(valid_refs) > 1:\n",
        "                    explanation.append(f\"Chunk {len(grouped_chunks)}: grouped {len(valid_refs)} mini-chunks\")\n",
        "                else:\n",
        "                    explanation.append(f\"Chunk {len(grouped_chunks)}: kept mini-chunk\")\n",
        "    return grouped_chunks, explanation\n",
        "\n"
      ],
      "metadata": {
        "id": "crEhiwEYlhNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# supabase setup + initialize vecs"
      ],
      "metadata": {
        "id": "4MThmT8bvzpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUd1BBe8OzkE"
      },
      "outputs": [],
      "source": [
        "!pip install supabase numpy psycopg2 --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KAWoGl2IT57"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIlUvi1eOzmY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import uuid\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize Supabase\n",
        "\n",
        "SUPABASE_URL =\n",
        "SUPABASE_KEY =\n",
        "\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Load Embedding Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eChWQCv6FAl3"
      },
      "outputs": [],
      "source": [
        "!pip install vecs --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4r4s57s2ybt"
      },
      "outputs": [],
      "source": [
        "import vecs\n",
        "from vecs.adapter import Adapter, ParagraphChunker, TextEmbedding\n",
        "\n",
        "DB_CONNECTION =\n",
        "\n",
        "vx = vecs.create_client(DB_CONNECTION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9XV0707EwOY"
      },
      "outputs": [],
      "source": [
        "vec_text = vx.get_or_create_collection(name=\"vec_text\", dimension=768)\n",
        "vec_table = vx.get_or_create_collection(name=\"vec_table\", dimension=768)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta6ExA1iNfSl"
      },
      "source": [
        "# embedding + store to DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwzQQDSALFxO"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "# embs = get_embedding(\"Uses of Machine learning\")\n",
        "# print(embs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T4qifmGO5Wb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_table_description(table_data):\n",
        "    \"\"\"Generates a natural language description from a table's headers and rows.\"\"\"\n",
        "    headers = table_data[\"headers\"]\n",
        "    rows = table_data[\"rows\"]\n",
        "\n",
        "    description = []\n",
        "    for row in rows:\n",
        "        row_text = \", \".join([f\"{headers[i]}: {row[i]}\" for i in range(len(headers))])\n",
        "        description.append(row_text)\n",
        "\n",
        "    return \" | \".join(description)  # Separate rows with \"|\"\n",
        "\n",
        "def convert_table_to_text(table_data, metadata):\n",
        "    \"\"\"Converts a table (headers + rows) into a structured text format with metadata and description for embedding.\"\"\"\n",
        "    headers = \", \".join(table_data[\"headers\"])\n",
        "    rows = [\" | \".join(row) for row in table_data[\"rows\"]]\n",
        "\n",
        "    # Retrieve metadata fields\n",
        "    table_title = metadata.get(\"table_title\", \"Unknown Table\")\n",
        "    section = metadata.get(\"section\", \"Unknown Section\")\n",
        "\n",
        "    # Generate description from table data\n",
        "    table_description = generate_table_description(table_data)\n",
        "\n",
        "    # Combine metadata with table content\n",
        "    return (\n",
        "        f\"Table Title: {table_title}. Section: {section}.\\n\"\n",
        "        f\"Table Data:\\nHeaders: {headers}\\n\" + \"\\n\".join(rows) +\n",
        "        f\"\\nDescription: {table_description}\"\n",
        "    ), table_description  # Return both formatted text & natural description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byemia7c1dg6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def store_chunks_in_supabase(chunks):\n",
        "    \"\"\"Stores text and table chunks into Supabase with improved embeddings.\"\"\"\n",
        "    document_entries = []\n",
        "    table_entries = []\n",
        "    text_records = []\n",
        "    table_records = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_id = str(uuid.uuid4())  # Generate unique chunk_id\n",
        "\n",
        "        # Process text content\n",
        "        if \"content\" in chunk and chunk[\"content\"]:\n",
        "            content = chunk[\"content\"]\n",
        "            embedding = get_embedding(content)\n",
        "\n",
        "            document_entries.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"content\": content,\n",
        "                \"metadata\": chunk[\"metadata\"],\n",
        "                \"type\": \"text\"\n",
        "            })\n",
        "            text_records.append((chunk_id, embedding, chunk[\"metadata\"]))\n",
        "\n",
        "        # Process table data\n",
        "        if \"table\" in chunk and chunk[\"table\"]:\n",
        "            table_data = chunk[\"table\"]\n",
        "            metadata = chunk.get(\"metadata\", {})\n",
        "\n",
        "            # Generate both structured table text & natural description\n",
        "            table_text, table_description = convert_table_to_text(table_data, metadata)\n",
        "            table_embedding = get_embedding(table_text)\n",
        "\n",
        "            table_entries.append({\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"table_data\": json.dumps(table_data, ensure_ascii=False),\n",
        "                # \"embedding\": table_embedding,\n",
        "                \"description\": table_description,  # Store the generated description\n",
        "                \"metadata\": metadata\n",
        "            })\n",
        "            table_records.append((chunk_id, table_embedding, metadata))\n",
        "\n",
        "    # Batch insert into Supabase\n",
        "    if document_entries:\n",
        "        supabase.table(\"documents\").insert(document_entries).execute()\n",
        "    if table_entries:\n",
        "        supabase.table(\"tables\").insert(table_entries).execute()\n",
        "\n",
        "    vec_text.upsert(records=text_records)\n",
        "    vec_table.upsert(records=table_records)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXpQofBaMf4u"
      },
      "source": [
        "# query embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYeEZR9cNT3_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_keywords_simple(text):\n",
        "    \"\"\"Extracts important words from a query using simple filtering.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\", \"average\", \"sum\", \"percentage\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def get_most_similar_keywords(query_keywords, top_text_chunks):\n",
        "    \"\"\"Extracts most relevant words from top retrieved text chunks.\"\"\"\n",
        "    all_text_words = set()\n",
        "    for chunk in top_text_chunks:\n",
        "        chunk_words = set(word_tokenize(chunk[2].lower()))  # Extract words from chunk text\n",
        "        all_text_words.update(chunk_words)\n",
        "    common_words = [word for word in query_keywords if word in all_text_words]\n",
        "    return common_words if common_words else query_keywords  # Fallback to original keywords if no match\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hybrid bm25 + vec"
      ],
      "metadata": {
        "id": "zBzBJQkHI4de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 --q"
      ],
      "metadata": {
        "id": "YqzyQpa2d3Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import psycopg2\n",
        "from scipy.spatial.distance import cosine\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Assume: get_embedding(), extract_keywords_simple(), query_requires_table() are already defined\n",
        "\n",
        "def hybrid_retrieve(user_query, all_chunks, top_k=10):\n",
        "    documents = [chunk[2] for chunk in all_chunks]  # chunk = (id, type, content, sim?)\n",
        "    tokenized_corpus = [doc.split() for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    bm25_scores = bm25.get_scores(user_query.split())\n",
        "\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        dense_sim = chunk[3] if chunk[3] else 0\n",
        "        sparse_score = bm25_scores[i] if bm25_scores[i] else 0\n",
        "        combined = 0.7 * dense_sim + 0.3 * sparse_score\n",
        "        all_chunks[i] = (*chunk, bm25_scores[i], combined)\n",
        "\n",
        "    all_chunks.sort(key=lambda x: x[5], reverse=True)  # sort by combined score\n",
        "    return all_chunks[:top_k]\n",
        "\n",
        "def query_supabase(user_query, top_k=5):\n",
        "    \"\"\"Hybrid Retrieval (BM25 + Dense Embedding) without reranking.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    keywords = extract_keywords_simple(user_query)\n",
        "    requires_table = query_requires_table(user_query)\n",
        "    query_list = query_embedding.tolist()\n",
        "\n",
        "    conn = psycopg2.connect(DB_CONNECTION)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    ##### TEXT CHUNKS #####\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, 1 - (vec <=> %s) AS similarity\n",
        "        FROM vecs.vec_text\n",
        "        ORDER BY vec <=> %s\n",
        "        LIMIT 10\n",
        "    \"\"\", (json.dumps(query_list), json.dumps(query_list)))\n",
        "    text_chunk_ids = cur.fetchall()\n",
        "\n",
        "    text_results = []\n",
        "    if text_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in text_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, content, metadata\n",
        "            FROM public.documents\n",
        "            WHERE chunk_id IN {chunk_id_list};\n",
        "        \"\"\")\n",
        "        text_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "        text_results = [(cid, \"text\", text_chunks[cid][0], sim) for cid, sim in text_chunk_ids if cid in text_chunks]\n",
        "\n",
        "    ##### TABLE CHUNKS #####\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, 1 - (vec <=> %s) AS similarity\n",
        "        FROM vecs.vec_table\n",
        "        ORDER BY vec <=> %s\n",
        "        LIMIT 10\n",
        "    \"\"\", (json.dumps(query_list), json.dumps(query_list)))\n",
        "    table_chunk_ids = cur.fetchall()\n",
        "\n",
        "    table_results = []\n",
        "    if table_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in table_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, description, metadata\n",
        "            FROM public.tables\n",
        "            WHERE chunk_id IN {chunk_id_list};\n",
        "        \"\"\")\n",
        "        table_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "        table_results = [(cid, \"table\", table_chunks[cid][0], sim) for cid, sim in table_chunk_ids if cid in table_chunks]\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    #### Combine Results and Run Hybrid ####\n",
        "    all_results = text_results + table_results\n",
        "    top_hybrid = hybrid_retrieve(user_query, all_results, top_k=top_k)\n",
        "\n",
        "    return top_hybrid\n"
      ],
      "metadata": {
        "id": "zb13Y1jZI3_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM function"
      ],
      "metadata": {
        "id": "U7mvRNuHoj_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API Key\n",
        "OPENAI_API_KEY =\n",
        "openai.api_key = OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "I2968Quh4YEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pd-BakFwzJD"
      },
      "outputs": [],
      "source": [
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "\n",
        "    # 🔹 Sanitize chat history (make sure all entries are dicts)\n",
        "    safe_history = []\n",
        "    for msg in chat_history:\n",
        "        if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n",
        "            safe_history.append(msg)\n",
        "        else:\n",
        "            print(\"⚠️ Skipping malformed chat history entry:\", msg)\n",
        "\n",
        "    # 🔹 Prepare context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # 🔹 Construct messages for OpenAI Chat API\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "        *safe_history,\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"}\n",
        "    ]\n",
        "\n",
        "    # 🔹 Make API call\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  # New client-style API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # 🔹 Extract and update chat history\n",
        "    answer = response.choices[0].message.content\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer, chat_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract NER function (spacy model) 🆕"
      ],
      "metadata": {
        "id": "YiFrBOJRgK-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm --q\n"
      ],
      "metadata": {
        "id": "mopkxpEUeHYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Load SpaCy NER model once\n",
        "_spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities(text: str) -> Dict[str, List[str]]:\n",
        "    entities = {}\n",
        "    doc = _spacy_nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        entities.setdefault(ent.label_, set()).add(ent.text)\n",
        "\n",
        "    # Convert sets to lists\n",
        "    return {k: list(v) for k, v in entities.items()}\n"
      ],
      "metadata": {
        "id": "SNs9TQ4WipCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contoh chat history (raw text) buat di extract entitynya\n",
        "\n",
        "hist = \"\"\"🔹 Input Query:\n",
        "explain the process of medical training\n",
        "\n",
        " 🔹 Chatbot Response:\n",
        " The process of medical training, particularly for family medicine in Africa as described in the provided context, involves a comprehensive, hands-on approach that takes place primarily in clinical settings such as clinics, district hospitals, or regional hospitals. Here is a summary of the key components of this training process:\n",
        "\n",
        "1. **Clinical Workplace Training**: Trainees receive their education directly at the bedside or next to the patient in various medical settings. This method emphasizes real-world experiences where direct patient contact is central to the learning process.\n",
        "\n",
        "2. **Supervised Learning**: An essential part of the training involves continuous direct supervision by senior medical professionals. Supervisors observe trainees during their interactions with patients and provide immediate feedback and reflection. This direct supervision facilitates an interactive learning environment crucial for developing practical skills.\n",
        "\n",
        "3. **Learning Tools and Assessment**: Various tools are employed to enhance and evaluate the learning process:\n",
        "   - **Mini-Clinical Evaluation Exercise (CEX)** and **Direct Observation of Procedural Skills (DOPS)** are used to assess specific skills during patient encounters.\n",
        "   - **Logbooks** track the progress and experiences of the trainees.\n",
        "   - **Global Assessment Tool** is used for broader evaluation, typically at the end of each rotation, involving both the trainee and the supervisor. This is complemented by continuous assessments every two weeks where trainees present topics and undergo quizzes.\n",
        "\n",
        "4. **Integrated Assessments**: The training incorporates both continuous assessments and specific periodical reviews (like quarterly assessments in some programs). These assessments often involve practical tests and reflection sessions that help in gauging the progress and understanding of the trainees.\n",
        "\n",
        "5. **Accreditation and Networked Training**: There is an emphasis on the accreditation of training facilities and creating a network of teaching campuses across different locations. This network allows for a diverse and comprehensive training experience, balancing between different hospital settings and primary health care environments.\n",
        "\n",
        "6. **Workshops and Conferences**: Participation in broader workshops and conferences, such as those organized by the World Organisation of Family Doctors (WONCA), also plays a role in training by providing additional learning opportunities and exposure to the global medical community.\n",
        "\n",
        "Overall, the training process is dynamic and interactive, focusing on direct patient care, continuous feedback, and structured assessments to ensure comprehensive learning and professional development in family medicine.\n",
        " \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(extract_entities(hist))"
      ],
      "metadata": {
        "id": "rdYoSvQkaMvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# context  (chat history + entity) and summarizer 🆕"
      ],
      "metadata": {
        "id": "cUlDEK3w3KUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "SUMMARIZER_MODEL_ID = \"t5-small\"\n",
        "summarizer_tokenizer = AutoTokenizer.from_pretrained(SUMMARIZER_MODEL_ID)\n",
        "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZER_MODEL_ID)\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "llm_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text):\n",
        "    return len(llm_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "woJsy5a_y6-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatContextManager:\n",
        "    def __init__(self, summarize_every_turns=3, summarize_every_tokens=1000, ner_model=\"spacy_sm\"):\n",
        "        self.chat_history: List[Dict] = []\n",
        "        self.summary: str = \"\"\n",
        "        self.turns_since_last_summary: int = 0\n",
        "        self.summarize_every_turns = summarize_every_turns\n",
        "        self.summarize_every_tokens = summarize_every_tokens\n",
        "        self.ner_model = ner_model\n",
        "\n",
        "    def summarize_text(self, text, max_input_tokens=512, max_output_tokens=150):\n",
        "        inputs = summarizer_tokenizer.encode(\n",
        "            \"summarize: \" + text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_input_tokens,\n",
        "            truncation=True\n",
        "        )\n",
        "        summary_ids = summarizer_model.generate(\n",
        "            inputs,\n",
        "            max_length=max_output_tokens,\n",
        "            min_length=30,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def add_turn(self, user_msg: str, bot_msg: str):\n",
        "        user_ents = extract_entities(user_msg, self.ner_model)\n",
        "        bot_ents = extract_entities(bot_msg, self.ner_model)\n",
        "\n",
        "        self.chat_history.append({\n",
        "            \"role\": \"user\",\n",
        "            \"message\": user_msg,\n",
        "            \"entities\": user_ents\n",
        "        })\n",
        "\n",
        "        self.chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"message\": bot_msg,\n",
        "            \"entities\": bot_ents\n",
        "        })\n",
        "\n",
        "        self.turns_since_last_summary += 1\n",
        "\n",
        "        if self.should_summarize():\n",
        "            self.update_summary()\n",
        "\n",
        "    def should_summarize(self):\n",
        "        full_text = \" \".join([turn[\"message\"] for turn in self.chat_history])\n",
        "        token_count = count_tokens(full_text)\n",
        "        return (\n",
        "            self.turns_since_last_summary >= self.summarize_every_turns\n",
        "            or token_count >= self.summarize_every_tokens\n",
        "        )\n",
        "\n",
        "    def update_summary(self):\n",
        "        full_text = \"\\n\".join([f\"{t['role']}: {t['message']}\" for t in self.chat_history])\n",
        "        new_summary = self.summarize_text(full_text)\n",
        "        print(\"\\n📝 Summary updated:\\n\", new_summary, \"\\n\")\n",
        "        self.summary = f\"{self.summary}\\n{new_summary}\" if self.summary else new_summary\n",
        "        self.chat_history = []\n",
        "        self.turns_since_last_summary = 0\n",
        "\n",
        "    def get_context_for_llm(self, recent_n=2):\n",
        "        recent_turns = self.chat_history[-recent_n * 2:]\n",
        "        recent_text = \"\\n\".join([f\"{t['role']}: {t['message']}\" for t in recent_turns])\n",
        "        return f\"Summary:\\n{self.summary}\\n\\nRecent Turns:\\n{recent_text}\"\n"
      ],
      "metadata": {
        "id": "2eWrgu1K1I9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test session history  🆕"
      ],
      "metadata": {
        "id": "ETfboWUTSEDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_ctx = ChatContextManager()\n",
        "\n",
        "print(\"Type '0' to exit, '1' to reset chat history. \\n\")\n",
        "\n",
        "while True:\n",
        "    print(\"\\n🔹🔹🔹🔹🔹🔹\\n\")\n",
        "    user_query = input(\"\\n🔹 Input Query:\\n\").strip()\n",
        "\n",
        "    if user_query == \"0\":\n",
        "        print(\"\\n exiting...\")\n",
        "        break\n",
        "\n",
        "    if user_query == \"1\":\n",
        "        chat_ctx = ChatContextManagerNER()  # reset context manager\n",
        "        print(\"\\n chat history cleared. \\n\")\n",
        "        continue\n",
        "\n",
        "    retrieved_chunks = query_supabase(user_query)\n",
        "    context = chat_ctx.get_context_for_llm()\n",
        "    chat_history = [{\"role\": \"user\", \"content\": context}] if context.strip() else []\n",
        "\n",
        "    try:\n",
        "        response, _ = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "        print(\"\\n 🔹 Chatbot Response:\\n\", response)\n",
        "        chat_ctx.add_turn(user_query, response)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n",
        "\n",
        "# explain the process of medical training"
      ],
      "metadata": {
        "id": "UYYyZEC-1M5w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2fp56lyvld1N",
        "4MThmT8bvzpU",
        "swq_HiH4EyEp",
        "ta6ExA1iNfSl",
        "OuT9m9Byy-3I",
        "MXpQofBaMf4u",
        "zBzBJQkHI4de",
        "mpe9QkFz_UyG",
        "U7mvRNuHoj_4",
        "20JVyBp9VfiX",
        "O_9FkBZxUeWe",
        "YiFrBOJRgK-4",
        "yFE62_o6Y6JK",
        "Nu3uRfnHY90k",
        "cUlDEK3w3KUw",
        "-KxZMPImzXtH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}