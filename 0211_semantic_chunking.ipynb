{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+5Cl2eKqopc6ksfU6AlLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessiechd/RAG_Model/blob/main/0211_semantic_chunking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt1Xf4PeBYOU",
        "outputId": "5798e200-61a8-4809-8bb6-8238442de090"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic chunker (SentenceTransformer)"
      ],
      "metadata": {
        "id": "sUaUF5itINRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class TextChunker:\n",
        "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v1'):\n",
        "        \"\"\"Initialize the TextChunker with a specified sentence transformer model.\"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def process_file(self, file_path, context_window=1, percentile_threshold=95, min_chunk_size=3):\n",
        "        \"\"\"\n",
        "        Process a text file and split it into semantically meaningful chunks.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the text file\n",
        "            context_window: Number of sentences to consider on either side for context\n",
        "            percentile_threshold: Percentile threshold for identifying breakpoints\n",
        "            min_chunk_size: Minimum number of sentences in a chunk\n",
        "\n",
        "        Returns:\n",
        "            list: Semantically coherent text chunks\n",
        "        \"\"\"\n",
        "        # Process the text file\n",
        "        sentences = self._load_text(file_path)\n",
        "        contextualized = self._add_context(sentences, context_window)\n",
        "        embeddings = self.model.encode(contextualized)\n",
        "\n",
        "        # Create and refine chunks\n",
        "        distances = self._calculate_distances(embeddings)\n",
        "        breakpoints = self._identify_breakpoints(distances, percentile_threshold)\n",
        "        initial_chunks = self._create_chunks(sentences, breakpoints)\n",
        "\n",
        "        # Merge small chunks for better coherence\n",
        "        chunk_embeddings = self.model.encode(initial_chunks)\n",
        "        final_chunks = self._merge_small_chunks(initial_chunks, chunk_embeddings, min_chunk_size)\n",
        "\n",
        "        return final_chunks\n",
        "\n",
        "    def _load_text(self, file_path):\n",
        "        \"\"\"Load and tokenize text from a file.\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "    def _add_context(self, sentences, window_size):\n",
        "        \"\"\"Combine sentences with their neighbors for better context.\"\"\"\n",
        "        contextualized = []\n",
        "        for i in range(len(sentences)):\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(sentences), i + window_size + 1)\n",
        "            context = ' '.join(sentences[start:end])\n",
        "            contextualized.append(context)\n",
        "        return contextualized\n",
        "\n",
        "    def _calculate_distances(self, embeddings):\n",
        "        \"\"\"Calculate cosine distances between consecutive embeddings.\"\"\"\n",
        "        distances = []\n",
        "        for i in range(len(embeddings) - 1):\n",
        "            similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "            distance = 1 - similarity\n",
        "            distances.append(distance)\n",
        "        return distances\n",
        "\n",
        "    def _identify_breakpoints(self, distances, threshold_percentile):\n",
        "        \"\"\"Find natural breaking points in the text based on semantic distances.\"\"\"\n",
        "        threshold = np.percentile(distances, threshold_percentile)\n",
        "        return [i for i, dist in enumerate(distances) if dist > threshold]\n",
        "\n",
        "    def _create_chunks(self, sentences, breakpoints):\n",
        "        \"\"\"Create initial text chunks based on identified breakpoints.\"\"\"\n",
        "        chunks = []\n",
        "        start_idx = 0\n",
        "\n",
        "        for breakpoint in breakpoints:\n",
        "            chunk = ' '.join(sentences[start_idx:breakpoint + 1])\n",
        "            chunks.append(chunk)\n",
        "            start_idx = breakpoint + 1\n",
        "\n",
        "        # Add the final chunk\n",
        "        final_chunk = ' '.join(sentences[start_idx:])\n",
        "        chunks.append(final_chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _merge_small_chunks(self, chunks, embeddings, min_size):\n",
        "        \"\"\"Merge small chunks with their most similar neighbor.\"\"\"\n",
        "        final_chunks = [chunks[0]]\n",
        "        merged_embeddings = [embeddings[0]]\n",
        "\n",
        "        for i in range(1, len(chunks) - 1):\n",
        "            current_chunk_size = len(chunks[i].split('. '))\n",
        "\n",
        "            if current_chunk_size < min_size:\n",
        "                # Calculate similarities\n",
        "                prev_similarity = cosine_similarity([embeddings[i]], [merged_embeddings[-1]])[0][0]\n",
        "                next_similarity = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "\n",
        "                if prev_similarity > next_similarity:\n",
        "                    # Merge with previous chunk\n",
        "                    final_chunks[-1] = f\"{final_chunks[-1]} {chunks[i]}\"\n",
        "                    merged_embeddings[-1] = (merged_embeddings[-1] + embeddings[i]) / 2\n",
        "                else:\n",
        "                    # Merge with next chunk\n",
        "                    chunks[i + 1] = f\"{chunks[i]} {chunks[i + 1]}\"\n",
        "                    embeddings[i + 1] = (embeddings[i] + embeddings[i + 1]) / 2\n",
        "            else:\n",
        "                final_chunks.append(chunks[i])\n",
        "                merged_embeddings.append(embeddings[i])\n",
        "\n",
        "        final_chunks.append(chunks[-1])\n",
        "        return final_chunks\n",
        "\n",
        "    def evaluate_coherence(self, chunks):\n",
        "        coherence_scores = []\n",
        "        embeddings = self.model.encode(chunks)\n",
        "        for i in range(len(embeddings) - 1):\n",
        "            score = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "            coherence_scores.append(score)\n",
        "        return np.mean(coherence_scores)\n"
      ],
      "metadata": {
        "id": "T36nXNLWBaQZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxJjf6enC9EK",
        "outputId": "127aa8b7-e8b6-449e-c694-347c8097f157"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Example usage of the TextChunker class.\"\"\"\n",
        "# Initialize the chunker\n",
        "chunker = TextChunker()\n",
        "\n",
        "# Process a text file\n",
        "file_path = \"/content/17_qwen1.md\"\n"
      ],
      "metadata": {
        "id": "RbkTuKu0Bmpa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunker.process_file(\n",
        "  file_path,\n",
        "  context_window=1,\n",
        "  percentile_threshold=95,\n",
        "  min_chunk_size=3\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"Successfully split text into {len(chunks)} chunks\")\n",
        "\n",
        "for i in range(len(chunks)):\n",
        "  print(f\"Chunk {i+1}: {len(chunks[i].split('. '))} sentences\")\n",
        "\n",
        "coherence_score = chunker.evaluate_coherence(chunks)\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_-_B15FEiLI",
        "outputId": "afd2601c-a521-40d5-8786-99c2306f8ce1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully split text into 10 chunks\n",
            "Chunk 1: 1 sentences\n",
            "Chunk 2: 22 sentences\n",
            "Chunk 3: 134 sentences\n",
            "Chunk 4: 14 sentences\n",
            "Chunk 5: 3 sentences\n",
            "Chunk 6: 5 sentences\n",
            "Chunk 7: 26 sentences\n",
            "Chunk 8: 11 sentences\n",
            "Chunk 9: 3 sentences\n",
            "Chunk 10: 10 sentences\n",
            "Coherence Score: 0.4878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunker.process_file(\n",
        "  file_path,\n",
        "  context_window=1,\n",
        "  percentile_threshold=85,\n",
        "  min_chunk_size=3\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"Successfully split text into {len(chunks)} chunks\")\n",
        "\n",
        "for i in range(len(chunks)):\n",
        "  print(f\"Chunk {i+1}: {len(chunks[i].split('. '))} sentences\")\n",
        "\n",
        "coherence_score = chunker.evaluate_coherence(chunks)\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZkewa0UE7_w",
        "outputId": "d38eca7b-1e62-4bef-948e-f867cd2009c8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully split text into 22 chunks\n",
            "Chunk 1: 1 sentences\n",
            "Chunk 2: 11 sentences\n",
            "Chunk 3: 5 sentences\n",
            "Chunk 4: 6 sentences\n",
            "Chunk 5: 111 sentences\n",
            "Chunk 6: 13 sentences\n",
            "Chunk 7: 8 sentences\n",
            "Chunk 8: 8 sentences\n",
            "Chunk 9: 4 sentences\n",
            "Chunk 10: 4 sentences\n",
            "Chunk 11: 3 sentences\n",
            "Chunk 12: 7 sentences\n",
            "Chunk 13: 8 sentences\n",
            "Chunk 14: 5 sentences\n",
            "Chunk 15: 5 sentences\n",
            "Chunk 16: 6 sentences\n",
            "Chunk 17: 3 sentences\n",
            "Chunk 18: 5 sentences\n",
            "Chunk 19: 3 sentences\n",
            "Chunk 20: 3 sentences\n",
            "Chunk 21: 8 sentences\n",
            "Chunk 22: 2 sentences\n",
            "Coherence Score: 0.4071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunker.process_file(\n",
        "  file_path,\n",
        "  context_window=2,\n",
        "  percentile_threshold=95,\n",
        "  min_chunk_size=3\n",
        ")\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Successfully split text into {len(chunks)} chunks\")\n",
        "\n",
        "for i in range(len(chunks)):\n",
        "  print(f\"Chunk {i+1}: {len(chunks[i].split('. '))} sentences\")\n",
        "\n",
        "coherence_score = chunker.evaluate_coherence(chunks)\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DPuqunoFiB1",
        "outputId": "b5bdd509-f57b-447c-e88f-4d721ef5c069"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully split text into 9 chunks\n",
            "Chunk 1: 14 sentences\n",
            "Chunk 2: 8 sentences\n",
            "Chunk 3: 38 sentences\n",
            "Chunk 4: 96 sentences\n",
            "Chunk 5: 12 sentences\n",
            "Chunk 6: 41 sentences\n",
            "Chunk 7: 5 sentences\n",
            "Chunk 8: 4 sentences\n",
            "Chunk 9: 11 sentences\n",
            "Coherence Score: 0.4426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Chunking with added ROUGE, QA score\n",
        "- added ROUGE and QA-based retrieval success score\n",
        "- added dynamic vs. fixed context windows (adaptive based on similarity)\n",
        "- added K-means clustering for breakpoints instead of thresholds\n",
        "- improved merging based on topic similarity\n",
        "- added chunks overlaps"
      ],
      "metadata": {
        "id": "dJUK_evyIGWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge --q"
      ],
      "metadata": {
        "id": "hoxlVlHFIWwW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import rouge\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class TextChunker2:\n",
        "    def __init__(self, model_name='sentence-transformers/all-mpnet-base-v1'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def process_file(self, file_path, dynamic_window=True, min_chunk_size=3, overlap=1, num_clusters=5):\n",
        "        sentences = self._load_text(file_path)\n",
        "        contextualized = self._add_dynamic_context(sentences) if dynamic_window else self._add_fixed_context(sentences)\n",
        "        embeddings = self.model.encode(contextualized)\n",
        "\n",
        "        # Identify breakpoints using KMeans clustering\n",
        "        breakpoints = self._identify_breakpoints_clustering(embeddings, num_clusters)\n",
        "        initial_chunks = self._create_chunks(sentences, breakpoints, overlap)\n",
        "\n",
        "        # Merge small chunks using topic modeling and similarity\n",
        "        chunk_embeddings = self.model.encode(initial_chunks)\n",
        "        final_chunks = self._merge_small_chunks(initial_chunks, chunk_embeddings, min_chunk_size)\n",
        "\n",
        "        return final_chunks\n",
        "\n",
        "    def _load_text(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return sent_tokenize(text)\n",
        "\n",
        "    def _add_fixed_context(self, sentences, window_size=1):\n",
        "        return [' '.join(sentences[max(0, i-window_size): min(len(sentences), i+window_size+1)]) for i in range(len(sentences))]\n",
        "\n",
        "    def _add_dynamic_context(self, sentences):\n",
        "        contextualized = []\n",
        "        embeddings = self.model.encode(sentences)\n",
        "        for i in range(len(sentences)):\n",
        "            similarities = cosine_similarity([embeddings[i]], embeddings)[0]\n",
        "            closest_indices = np.argsort(-similarities)[:3]  # Select 2 most relevant neighbors\n",
        "            context = ' '.join(sentences[j] for j in sorted(closest_indices))\n",
        "            contextualized.append(context)\n",
        "        return contextualized\n",
        "\n",
        "    def _identify_breakpoints_clustering(self, embeddings, num_clusters):\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10).fit(embeddings)\n",
        "        labels = kmeans.labels_\n",
        "        return [i for i in range(1, len(labels)) if labels[i] != labels[i-1]]\n",
        "\n",
        "    def _create_chunks(self, sentences, breakpoints, overlap):\n",
        "        chunks, start_idx = [], 0\n",
        "        for breakpoint in breakpoints:\n",
        "            end_idx = breakpoint + 1\n",
        "            chunk = ' '.join(sentences[max(0, start_idx - overlap): end_idx])\n",
        "            chunks.append(chunk)\n",
        "            start_idx = end_idx\n",
        "        chunks.append(' '.join(sentences[max(0, start_idx - overlap):]))\n",
        "        return chunks\n",
        "\n",
        "    def _merge_small_chunks(self, chunks, embeddings, min_size):\n",
        "        final_chunks, merged_embeddings = [chunks[0]], [embeddings[0]]\n",
        "        for i in range(1, len(chunks) - 1):\n",
        "            if len(chunks[i].split('. ')) < min_size:\n",
        "                prev_sim = cosine_similarity([embeddings[i]], [merged_embeddings[-1]])[0][0]\n",
        "                next_sim = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "                if prev_sim > next_sim:\n",
        "                    final_chunks[-1] += ' ' + chunks[i]\n",
        "                    merged_embeddings[-1] = (merged_embeddings[-1] + embeddings[i]) / 2\n",
        "                else:\n",
        "                    chunks[i + 1] = chunks[i] + ' ' + chunks[i + 1]\n",
        "                    embeddings[i + 1] = (embeddings[i] + embeddings[i + 1]) / 2\n",
        "            else:\n",
        "                final_chunks.append(chunks[i])\n",
        "                merged_embeddings.append(embeddings[i])\n",
        "        final_chunks.append(chunks[-1])\n",
        "        return final_chunks\n",
        "\n",
        "    def evaluate_coherence(self, chunks):\n",
        "        coherence_scores = []\n",
        "        embeddings = self.model.encode(chunks)\n",
        "        for i in range(len(embeddings) - 1):\n",
        "            score = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
        "            coherence_scores.append(score)\n",
        "        return np.mean(coherence_scores)\n",
        "\n",
        "    def evaluate_rouge(self, original_text, chunks):\n",
        "        rouge_evaluator = rouge.Rouge()\n",
        "        scores = [rouge_evaluator.get_scores(chunk, original_text)[0]['rouge-1']['f'] for chunk in chunks]\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def evaluate_qa_performance(self, retrieval_system, test_questions):\n",
        "        correct, total = 0, len(test_questions)\n",
        "        for question, expected_answer in test_questions:\n",
        "            retrieved_chunk = retrieval_system.retrieve(question)\n",
        "            if expected_answer in retrieved_chunk:\n",
        "                correct += 1\n",
        "        return correct / total\n"
      ],
      "metadata": {
        "id": "PSb1Qq2vIIn0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "p9H7ywMINxGf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRetrieval:\n",
        "    def __init__(self, chunks):\n",
        "        self.chunks = chunks\n",
        "        self.embeddings = chunker.model.encode(chunks)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        query_embedding = chunker.model.encode([query])\n",
        "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "        best_chunk = self.chunks[np.argmax(similarities)]\n",
        "        return best_chunk\n",
        "\n",
        "retrieval_system = SimpleRetrieval(chunks)"
      ],
      "metadata": {
        "id": "oXTfkVGVOH8H"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = [\n",
        "    (\"What is this text about?\", \"Family medicine training in Africa\"),\n",
        "    (\"Who is involved in the training?\", \"Department of Family and Emergency Medicine\"),\n",
        "]"
      ],
      "metadata": {
        "id": "ma4jEs7oON27"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunker2 = TextChunker2()\n",
        "chunks2 = chunker2.process_file(\n",
        "    file_path,\n",
        "    dynamic_window=True,\n",
        "    min_chunk_size=3,\n",
        "    overlap=1,\n",
        "    num_clusters=3)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Successfully split text into {len(chunks2)} chunks\")\n",
        "\n",
        "for i in range(len(chunks2)):\n",
        "  print(f\"Chunk {i+1}: {len(chunks2[i].split('. '))} sentences\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AE_23W9RIooI",
        "outputId": "30756fa9-b5ea-4bc4-f701-70c6702bb930"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully split text into 61 chunks\n",
            "Chunk 1: 4 sentences\n",
            "Chunk 2: 3 sentences\n",
            "Chunk 3: 6 sentences\n",
            "Chunk 4: 5 sentences\n",
            "Chunk 5: 4 sentences\n",
            "Chunk 6: 13 sentences\n",
            "Chunk 7: 3 sentences\n",
            "Chunk 8: 6 sentences\n",
            "Chunk 9: 5 sentences\n",
            "Chunk 10: 6 sentences\n",
            "Chunk 11: 8 sentences\n",
            "Chunk 12: 4 sentences\n",
            "Chunk 13: 3 sentences\n",
            "Chunk 14: 3 sentences\n",
            "Chunk 15: 3 sentences\n",
            "Chunk 16: 3 sentences\n",
            "Chunk 17: 5 sentences\n",
            "Chunk 18: 5 sentences\n",
            "Chunk 19: 5 sentences\n",
            "Chunk 20: 4 sentences\n",
            "Chunk 21: 6 sentences\n",
            "Chunk 22: 4 sentences\n",
            "Chunk 23: 3 sentences\n",
            "Chunk 24: 4 sentences\n",
            "Chunk 25: 4 sentences\n",
            "Chunk 26: 4 sentences\n",
            "Chunk 27: 3 sentences\n",
            "Chunk 28: 8 sentences\n",
            "Chunk 29: 3 sentences\n",
            "Chunk 30: 11 sentences\n",
            "Chunk 31: 43 sentences\n",
            "Chunk 32: 6 sentences\n",
            "Chunk 33: 5 sentences\n",
            "Chunk 34: 7 sentences\n",
            "Chunk 35: 3 sentences\n",
            "Chunk 36: 4 sentences\n",
            "Chunk 37: 5 sentences\n",
            "Chunk 38: 4 sentences\n",
            "Chunk 39: 6 sentences\n",
            "Chunk 40: 4 sentences\n",
            "Chunk 41: 6 sentences\n",
            "Chunk 42: 3 sentences\n",
            "Chunk 43: 4 sentences\n",
            "Chunk 44: 4 sentences\n",
            "Chunk 45: 4 sentences\n",
            "Chunk 46: 5 sentences\n",
            "Chunk 47: 5 sentences\n",
            "Chunk 48: 4 sentences\n",
            "Chunk 49: 5 sentences\n",
            "Chunk 50: 3 sentences\n",
            "Chunk 51: 5 sentences\n",
            "Chunk 52: 4 sentences\n",
            "Chunk 53: 3 sentences\n",
            "Chunk 54: 7 sentences\n",
            "Chunk 55: 4 sentences\n",
            "Chunk 56: 3 sentences\n",
            "Chunk 57: 4 sentences\n",
            "Chunk 58: 4 sentences\n",
            "Chunk 59: 5 sentences\n",
            "Chunk 60: 5 sentences\n",
            "Chunk 61: 2 sentences\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-278f8404dbfc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcoherence_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunker2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunker2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_rouge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mqa_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunker2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_qa_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieval_system\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Coherence Score: {coherence_score:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-d8a472952909>\u001b[0m in \u001b[0;36mevaluate_qa_performance\u001b[0;34m(self, retrieval_system, test_questions)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_qa_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieval_system\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_answer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_questions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mretrieved_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieval_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexpected_answer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretrieved_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_score = chunker2.evaluate_coherence(chunks2)\n",
        "rouge_score = chunker2.evaluate_rouge(text, chunks2)\n",
        "qa_accuracy = chunker2.evaluate_qa_performance(retrieval_system, test_questions)\n",
        "\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")\n",
        "print(f\"ROUGE Score: {rouge_score:.4f}\")\n",
        "print(f\"QA Accuracy: {qa_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JWRNFdFRgps",
        "outputId": "7e383a76-ac97-4651-96eb-f2ffe2eb6a50"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.6663\n",
            "ROUGE Score: 0.0771\n",
            "QA Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunker2 = TextChunker2()\n",
        "chunks2 = chunker2.process_file(\n",
        "    file_path,\n",
        "    dynamic_window=True,\n",
        "    min_chunk_size=20,\n",
        "    overlap=5,\n",
        "    num_clusters=3)\n",
        "\n",
        "# Print results\n",
        "print(f\"Successfully split text into {len(chunks2)} chunks\")\n",
        "\n",
        "for i in range(len(chunks2)):\n",
        "  print(f\"Chunk {i+1}: {len(chunks2[i].split('. '))} sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "880WdlZqQcJx",
        "outputId": "31e12976-842d-47c9-a01d-244cf8cf17bc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully split text into 21 chunks\n",
            "Chunk 1: 9 sentences\n",
            "Chunk 2: 36 sentences\n",
            "Chunk 3: 20 sentences\n",
            "Chunk 4: 62 sentences\n",
            "Chunk 5: 21 sentences\n",
            "Chunk 6: 43 sentences\n",
            "Chunk 7: 27 sentences\n",
            "Chunk 8: 62 sentences\n",
            "Chunk 9: 34 sentences\n",
            "Chunk 10: 51 sentences\n",
            "Chunk 11: 23 sentences\n",
            "Chunk 12: 22 sentences\n",
            "Chunk 13: 26 sentences\n",
            "Chunk 14: 35 sentences\n",
            "Chunk 15: 26 sentences\n",
            "Chunk 16: 39 sentences\n",
            "Chunk 17: 23 sentences\n",
            "Chunk 18: 35 sentences\n",
            "Chunk 19: 36 sentences\n",
            "Chunk 20: 34 sentences\n",
            "Chunk 21: 8 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_score = chunker2.evaluate_coherence(chunks2)\n",
        "rouge_score = chunker2.evaluate_rouge(text, chunks2)\n",
        "qa_accuracy = chunker2.evaluate_qa_performance(retrieval_system, test_questions)\n",
        "\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")\n",
        "print(f\"ROUGE Score: {rouge_score:.4f}\")\n",
        "print(f\"QA Accuracy: {qa_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWFavMzRR3NI",
        "outputId": "6d8e6fcb-9432-41e8-ca4d-f1f009036382"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.7323\n",
            "ROUGE Score: 0.1941\n",
            "QA Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunker2 = TextChunker2()\n",
        "chunks2 = chunker2.process_file(\n",
        "    file_path,\n",
        "    dynamic_window=True,\n",
        "    min_chunk_size=40,\n",
        "    overlap=5,\n",
        "    num_clusters=3)\n",
        "\n",
        "# Print results\n",
        "print(f\"Successfully split text into {len(chunks2)} chunks\")\n",
        "\n",
        "for i in range(len(chunks2)):\n",
        "  print(f\"Chunk {i+1}: {len(chunks2[i].split('. '))} sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_81YaioPVfvc",
        "outputId": "ec6b5933-0539-4401-a80c-c9e66c56df22"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully split text into 11 chunks\n",
            "Chunk 1: 9 sentences\n",
            "Chunk 2: 56 sentences\n",
            "Chunk 3: 62 sentences\n",
            "Chunk 4: 64 sentences\n",
            "Chunk 5: 43 sentences\n",
            "Chunk 6: 80 sentences\n",
            "Chunk 7: 51 sentences\n",
            "Chunk 8: 45 sentences\n",
            "Chunk 9: 61 sentences\n",
            "Chunk 10: 159 sentences\n",
            "Chunk 11: 41 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_score = chunker2.evaluate_coherence(chunks2)\n",
        "rouge_score = chunker2.evaluate_rouge(text, chunks2)\n",
        "qa_accuracy = chunker2.evaluate_qa_performance(retrieval_system, test_questions)\n",
        "\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")\n",
        "print(f\"ROUGE Score: {rouge_score:.4f}\")\n",
        "print(f\"QA Accuracy: {qa_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukRoqsjQVkPh",
        "outputId": "83c523d5-fd20-42c4-c3e7-39da7a8e0fe4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.6473\n",
            "ROUGE Score: 0.2869\n",
            "QA Accuracy: 0.00%\n"
          ]
        }
      ]
    }
  ]
}