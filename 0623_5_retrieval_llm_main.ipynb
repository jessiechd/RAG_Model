{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzTh5li1KbS3JEFCUx+ZWF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessiechd/RAG_Model/blob/main/0623_5_retrieval_llm_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG: hybrid retrieval + reranker BGE\n",
        "\n",
        "- ada beberapa tambahan function\n",
        "- perubahan di function ```query_supabase``` (2 algoritma query baru yaitu ```query_supabase_hybrid``` dan ```query_supabase_bge```)"
      ],
      "metadata": {
        "id": "TtcI_zPOAck1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reqs"
      ],
      "metadata": {
        "id": "EiKvZJs-7y6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase numpy psycopg2 vecs --q\n",
        "!pip install torch transformers openai python-dotenv --q\n",
        "!pip install scipy nltk fastapi uvicorn pgvector==0.3.2 psycopg2-binary --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab9A1GIIaADV",
        "outputId": "cc853a84-4177-430a-d6a4-90b4c0ad8eea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BrfiIOzDPCJD",
        "outputId": "a258be9b-6cf5-4c12-f7dc-d8781543fde8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-4132982602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0menv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\".env\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mSUPABASE_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SUPABASE_URL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import uuid\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import ast\n",
        "import re\n",
        "import vecs\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import psycopg2\n",
        "from pgvector.psycopg2 import register_vector\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "env_path = Path(__file__).resolve().parents[1] / \".env\"\n",
        "load_dotenv(dotenv_path=env_path)\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "DB_CONNECTION = os.getenv(\"DB_CONNECTION\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "SUPABASE_BUCKET = os.getenv(\"SUPABASE_BUCKET\")\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "vx = vecs.create_client(DB_CONNECTION)\n",
        "vec_text = vx.get_or_create_collection(name=\"vec_text\", dimension=768)\n",
        "vec_table = vx.get_or_create_collection(name=\"vec_table\", dimension=768)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True).to(\n",
        "    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def get_accessible_session_ids(supabase: Client, user_id: str):\n",
        "    \"\"\"Mengambil ID session yang dapat diakses oleh user berdasarkan role dan aturan session.\"\"\"\n",
        "\n",
        "    user_data = (\n",
        "        supabase.table(\"users\")\n",
        "        .select(\"user_role, is_admin\")\n",
        "        .eq(\"id\", user_id)\n",
        "        .single()\n",
        "        .execute()\n",
        "    )\n",
        "\n",
        "    if user_data.data is None:\n",
        "        return []\n",
        "\n",
        "    user_role = user_data.data[\"user_role\"]\n",
        "    is_admin = user_data.data[\"is_admin\"]\n",
        "\n",
        "    if is_admin:\n",
        "        sessions = supabase.table(\"sessions\").select(\"id\").execute()\n",
        "        if sessions.data is None:\n",
        "            return []\n",
        "        return [s[\"id\"] for s in sessions.data]\n",
        "\n",
        "    sessions = supabase.table(\"sessions\").select(\"id, is_public, allowed_roles, created_by\").execute()\n",
        "    if sessions.data is None:\n",
        "        return []\n",
        "\n",
        "    accessible_ids = []\n",
        "\n",
        "    for session in sessions.data:\n",
        "        allowed_roles = session.get(\"allowed_roles\", [])\n",
        "        if isinstance(allowed_roles, str):\n",
        "            allowed_roles = allowed_roles.split(\",\")\n",
        "        if (\n",
        "            session[\"is_public\"]\n",
        "            or session[\"created_by\"] == user_id\n",
        "            or user_role in allowed_roles\n",
        "        ):\n",
        "            accessible_ids.append(session[\"id\"])\n",
        "\n",
        "    return accessible_ids\n",
        "\n",
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "    print(\"\\n[DEBUG] Context sent to LLM:\")\n",
        "    print(context_text[:500])\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "    ]\n",
        "    messages.extend(chat_history)\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"})\n",
        "    client = openai.OpenAI(api_key=openai.api_key)\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "    return answer, chat_history\n",
        "\n",
        "def chat():\n",
        "    \"\"\"Handles continuous chat interaction with support for new chat and conversational context.\"\"\"\n",
        "    chat_history = []\n",
        "    print(\"Welcome to the assistant! Type 'exit' to end the chat, 'new chat' to start over.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"User: \")\n",
        "\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        if user_query.lower() == \"new chat\":\n",
        "            chat_history = []\n",
        "            print(\"Starting a new chat...\\n\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        retrieved_chunks = query_supabase(user_query)\n",
        "\n",
        "        answer, chat_history = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "\n",
        "        print(f\"Assistant: {answer}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query_supabase (current model)"
      ],
      "metadata": {
        "id": "1pS9iIW1ajxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_supabase(user_query, user_id, session_ids=None):\n",
        "    query_embedding = get_embedding(user_query)\n",
        "    embedding_str = ','.join([str(x) for x in query_embedding])\n",
        "\n",
        "    conn = psycopg2.connect(DB_CONNECTION)\n",
        "    register_vector(conn)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    TOP_K = 20\n",
        "\n",
        "    query_text = f\"\"\"\n",
        "        SELECT id, 1 - (vec <=> ARRAY[{embedding_str}]::vector) AS similarity\n",
        "        FROM vecs.vec_text\n",
        "        ORDER BY vec <=> ARRAY[{embedding_str}]::vector\n",
        "        LIMIT {TOP_K}\n",
        "    \"\"\"\n",
        "    cur.execute(query_text)\n",
        "    text_chunk_ids = cur.fetchall()\n",
        "\n",
        "    text_results = []\n",
        "    if text_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in text_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, content, metadata, session_id\n",
        "            FROM public.documents_chunk\n",
        "            WHERE chunk_id IN %s;\n",
        "        \"\"\", (chunk_id_list,))\n",
        "        text_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "\n",
        "        for cid, sim in text_chunk_ids:\n",
        "            if cid in text_chunks:\n",
        "                chunk = text_chunks[cid]\n",
        "                session_id = chunk[2]\n",
        "                if not session_ids or session_id in session_ids:\n",
        "                    text_results.append((cid, \"text\", chunk[0], sim))\n",
        "\n",
        "    query_table = f\"\"\"\n",
        "        SELECT id, 1 - (vec <=> ARRAY[{embedding_str}]::vector) AS similarity\n",
        "        FROM vecs.vec_table\n",
        "        ORDER BY vec <=> ARRAY[{embedding_str}]::vector\n",
        "        LIMIT {TOP_K}\n",
        "    \"\"\"\n",
        "    cur.execute(query_table)\n",
        "    table_chunk_ids = cur.fetchall()\n",
        "\n",
        "    table_results = []\n",
        "    if table_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in table_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, description, metadata, session_id\n",
        "            FROM public.tables_chunk\n",
        "            WHERE chunk_id IN %s;\n",
        "        \"\"\", (chunk_id_list,))\n",
        "        table_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "\n",
        "        for cid, sim in table_chunk_ids:\n",
        "            if cid in table_chunks:\n",
        "                chunk = table_chunks[cid]\n",
        "                session_id = chunk[2]\n",
        "                if not session_ids or session_id in session_ids:\n",
        "                    table_results.append((cid, \"table\", chunk[0], sim))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    combined_results = text_results + table_results\n",
        "    combined_results.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "    return combined_results[:5]"
      ],
      "metadata": {
        "id": "MiK_10gHaay6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query_supabase_hybrid (hybrid vecs + bm25)\n",
        "- function query_supabase with updated similarity (70% embedding similarity + 30% BM25 similarity)\n",
        "- BM25 similarity = keyword search algorithm; good for answering short queries (short sentences, simple phrases)\n",
        "\n",
        "- **updates:**\n",
        "  1. penambahan function ``` hybrid_retrieve ```\n",
        "  2. perubahan di bagian akhir function ```query_supabase```:\n",
        "\n",
        "   ```combined_results = hybrid_retrieve(user_query, combined_results)```\n",
        "\n"
      ],
      "metadata": {
        "id": "WHHDdnYLbNcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import psycopg2\n",
        "from scipy.spatial.distance import cosine\n",
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "LWiREtXx1aSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_retrieve(user_query, all_chunks, top_k=10):\n",
        "    documents = [chunk[2] for chunk in all_chunks]\n",
        "    tokenized_corpus = [doc.split() for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    bm25_scores = bm25.get_scores(user_query.split())\n",
        "\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        dense_sim = chunk[3] if chunk[3] else 0\n",
        "        sparse_score = bm25_scores[i] if bm25_scores[i] else 0\n",
        "        combined = 0.7 * dense_sim + 0.3 * sparse_score   # change similarity ratio\n",
        "        all_chunks[i] = (*chunk, bm25_scores[i], combined)\n",
        "\n",
        "    all_chunks.sort(key=lambda x: x[5], reverse=True)  # sort by combined score\n",
        "    return all_chunks[:top_k]"
      ],
      "metadata": {
        "id": "ulLTPOT71cgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_supabase_hybrid(user_query, user_id, session_ids=None):\n",
        "    query_embedding = get_embedding(user_query)\n",
        "    embedding_str = ','.join([str(x) for x in query_embedding])\n",
        "\n",
        "    conn = psycopg2.connect(DB_CONNECTION)\n",
        "    register_vector(conn)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    TOP_K = 20\n",
        "\n",
        "    query_text = f\"\"\"\n",
        "        SELECT id, 1 - (vec <=> ARRAY[{embedding_str}]::vector) AS similarity\n",
        "        FROM vecs.vec_text\n",
        "        ORDER BY vec <=> ARRAY[{embedding_str}]::vector\n",
        "        LIMIT {TOP_K}\n",
        "    \"\"\"\n",
        "    cur.execute(query_text)\n",
        "    text_chunk_ids = cur.fetchall()\n",
        "\n",
        "    text_results = []\n",
        "    if text_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in text_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, content, metadata, session_id\n",
        "            FROM public.documents_chunk\n",
        "            WHERE chunk_id IN %s;\n",
        "        \"\"\", (chunk_id_list,))\n",
        "        text_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "\n",
        "        for cid, sim in text_chunk_ids:\n",
        "            if cid in text_chunks:\n",
        "                chunk = text_chunks[cid]\n",
        "                session_id = chunk[2]\n",
        "                if not session_ids or session_id in session_ids:\n",
        "                    text_results.append((cid, \"text\", chunk[0], sim))\n",
        "\n",
        "    query_table = f\"\"\"\n",
        "        SELECT id, 1 - (vec <=> ARRAY[{embedding_str}]::vector) AS similarity\n",
        "        FROM vecs.vec_table\n",
        "        ORDER BY vec <=> ARRAY[{embedding_str}]::vector\n",
        "        LIMIT {TOP_K}\n",
        "    \"\"\"\n",
        "    cur.execute(query_table)\n",
        "    table_chunk_ids = cur.fetchall()\n",
        "\n",
        "    table_results = []\n",
        "    if table_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in table_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, description, metadata, session_id\n",
        "            FROM public.tables_chunk\n",
        "            WHERE chunk_id IN %s;\n",
        "        \"\"\", (chunk_id_list,))\n",
        "        table_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "\n",
        "        for cid, sim in table_chunk_ids:\n",
        "            if cid in table_chunks:\n",
        "                chunk = table_chunks[cid]\n",
        "                session_id = chunk[2]\n",
        "                if not session_ids or session_id in session_ids:\n",
        "                    table_results.append((cid, \"table\", chunk[0], sim))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    combined_results = text_results + table_results\n",
        "    combined_results = hybrid_retrieve(user_query, combined_results)\n",
        "\n",
        "    return combined_results[:5]"
      ],
      "metadata": {
        "id": "pO1NtpNybbzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query_supabase_bge (BGE reranker)\n",
        "\n",
        "- function query_supabase with updated hybrid similarity (70% embedding similarity + 30% BM25 similarity) and BGE reranker\n",
        "- ``` hybrid_retrieve ``` sama seperti yang digunakan di ```query_supabase_hybrid```\n",
        "\n",
        "- **perbedaan dari query_supabase_hybrid:**\n",
        "  1. penambahan function ``` rerank_with_bge ``` (model cross-encoder for reranking retrieval results based on semantic similarity and textual data)\n",
        "  2. penambahan function ``` rewrite_query ``` (rewrite query using TinyLlama agar lebih rapi dan embeddingnya bisa lebih terstruktur untuk sistem retrieval)\n",
        "  3. perubahan di bagian akhir function ```query_supabase```:\n",
        "\n",
        "   codeblock with comment ```### Optional```, can be implemented if computing resources are enough (BGE reranker lumayan lama processingnya. mungkin bisa di implementasi pada saat user meng-query konteks library saja)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F030Wwo0bY5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 --q"
      ],
      "metadata": {
        "id": "pWt9CSxS6-TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import psycopg2\n",
        "from scipy.spatial.distance import cosine\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# --- BGE Reranker ---\n",
        "def rerank_with_bge(query, chunks):\n",
        "    model_name = \"BAAI/bge-reranker-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    rerank_inputs = [\n",
        "        (query, chunk[2]) for chunk in chunks  # chunk[2] = content\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        [f\"{q} [SEP] {p}\" for q, p in rerank_inputs],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        scores = model(**inputs).logits.squeeze().tolist()\n",
        "\n",
        "    if isinstance(scores, float):\n",
        "        scores = [scores]\n",
        "\n",
        "    reranked_chunks = [(*chunk, score) for chunk, score in zip(chunks, scores)]\n",
        "    reranked_chunks.sort(key=lambda x: x[-1], reverse=True)\n",
        "    return reranked_chunks\n",
        "\n",
        "# --- TinyLlama Query Rewriter ---\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def rewrite_query(original_query):\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful query rewriter. Improve the following search query for a document retrieval system:\n",
        "Original: {original_query}\n",
        "Improved:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=30)\n",
        "    rewritten = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the improved part after 'Improved:'\n",
        "    if \"Improved:\" in rewritten:\n",
        "        rewritten = rewritten.split(\"Improved:\", 1)[1].strip()\n",
        "    return rewritten"
      ],
      "metadata": {
        "id": "Vksm2tZO7gww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_supabase(user_query, user_id, session_ids=None):\n",
        "    query_embedding = get_embedding(user_query)\n",
        "    embedding_str = ','.join([str(x) for x in query_embedding])\n",
        "\n",
        "    conn = psycopg2.connect(DB_CONNECTION)\n",
        "    register_vector(conn)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    TOP_K = 20\n",
        "\n",
        "    query_text = f\"\"\"\n",
        "        SELECT id, 1 - (vec <=> ARRAY[{embedding_str}]::vector) AS similarity\n",
        "        FROM vecs.vec_text\n",
        "        ORDER BY vec <=> ARRAY[{embedding_str}]::vector\n",
        "        LIMIT {TOP_K}\n",
        "    \"\"\"\n",
        "    cur.execute(query_text)\n",
        "    text_chunk_ids = cur.fetchall()\n",
        "\n",
        "    text_results = []\n",
        "    if text_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in text_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, content, metadata, session_id\n",
        "            FROM public.documents_chunk\n",
        "            WHERE chunk_id IN %s;\n",
        "        \"\"\", (chunk_id_list,))\n",
        "        text_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "\n",
        "        for cid, sim in text_chunk_ids:\n",
        "            if cid in text_chunks:\n",
        "                chunk = text_chunks[cid]\n",
        "                session_id = chunk[2]\n",
        "                if not session_ids or session_id in session_ids:\n",
        "                    text_results.append((cid, \"text\", chunk[0], sim))\n",
        "\n",
        "    query_table = f\"\"\"\n",
        "        SELECT id, 1 - (vec <=> ARRAY[{embedding_str}]::vector) AS similarity\n",
        "        FROM vecs.vec_table\n",
        "        ORDER BY vec <=> ARRAY[{embedding_str}]::vector\n",
        "        LIMIT {TOP_K}\n",
        "    \"\"\"\n",
        "    cur.execute(query_table)\n",
        "    table_chunk_ids = cur.fetchall()\n",
        "\n",
        "    table_results = []\n",
        "    if table_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in table_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, description, metadata, session_id\n",
        "            FROM public.tables_chunk\n",
        "            WHERE chunk_id IN %s;\n",
        "        \"\"\", (chunk_id_list,))\n",
        "        table_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "\n",
        "        for cid, sim in table_chunk_ids:\n",
        "            if cid in table_chunks:\n",
        "                chunk = table_chunks[cid]\n",
        "                session_id = chunk[2]\n",
        "                if not session_ids or session_id in session_ids:\n",
        "                    table_results.append((cid, \"table\", chunk[0], sim))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    combined_results = text_results + table_results\n",
        "    #### Optional: Query Expansion using TinyLlama ####\n",
        "    expanded_query = rewrite_query(user_query)\n",
        "    if expanded_query:\n",
        "        user_query = expanded_query\n",
        "\n",
        "    #### Hybrid Retrieval Scoring (BM25 + Dense) ####\n",
        "    top_hybrid = hybrid_retrieve(user_query, combined_results, top_k=top_k * 2)\n",
        "\n",
        "    #### Optional: Reranking using bge-reranker ####\n",
        "    reranked = rerank_with_bge(user_query, top_hybrid)\n",
        "    final_results = reranked[:top_k]\n",
        "\n",
        "    return final_results[:5]"
      ],
      "metadata": {
        "id": "-88oSz4tbZ8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main program"
      ],
      "metadata": {
        "id": "NbZZ8NPubNSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7C9fP3p5aczt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}