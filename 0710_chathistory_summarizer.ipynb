{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jessiechd/RAG_Model/blob/main/0710_chathistory_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuT9m9Byy-3I"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygLQrkHauqUi"
      },
      "outputs": [],
      "source": [
        "!pip install supabase numpy psycopg2 --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtTo1NiIwTuf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import uuid\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize Supabase\n",
        "SUPABASE_URL = \"\"\n",
        "SUPABASE_KEY = \"\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Load Embedding Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXpQofBaMf4u"
      },
      "source": [
        "# get embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYeEZR9cNT3_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding vector from input text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "\n",
        "def extract_keywords_simple(text):\n",
        "    \"\"\"Extracts important words from a query using simple filtering.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    keywords = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "def query_requires_table(user_query):\n",
        "    \"\"\"Determines if the query is likely asking for table data.\"\"\"\n",
        "    table_keywords = {\"table\", \"data\", \"values\", \"measurements\", \"limits\", \"thresholds\", \"parameters\", \"average\", \"sum\", \"percentage\"}\n",
        "    return any(word in user_query.lower() for word in table_keywords)\n",
        "\n",
        "def get_most_similar_keywords(query_keywords, top_text_chunks):\n",
        "    \"\"\"Extracts most relevant words from top retrieved text chunks.\"\"\"\n",
        "    all_text_words = set()\n",
        "    for chunk in top_text_chunks:\n",
        "        chunk_words = set(word_tokenize(chunk[2].lower()))  # Extract words from chunk text\n",
        "        all_text_words.update(chunk_words)\n",
        "    common_words = [word for word in query_keywords if word in all_text_words]\n",
        "    return common_words if common_words else query_keywords  # Fallback to original keywords if no match\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hybrid bm25 + vec"
      ],
      "metadata": {
        "id": "zBzBJQkHI4de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25 --q"
      ],
      "metadata": {
        "id": "YqzyQpa2d3Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import psycopg2\n",
        "from scipy.spatial.distance import cosine\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Assume: get_embedding(), extract_keywords_simple(), query_requires_table() are already defined\n",
        "\n",
        "def hybrid_retrieve(user_query, all_chunks, top_k=10):\n",
        "    documents = [chunk[2] for chunk in all_chunks]  # chunk = (id, type, content, sim?)\n",
        "    tokenized_corpus = [doc.split() for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    bm25_scores = bm25.get_scores(user_query.split())\n",
        "\n",
        "    for i, chunk in enumerate(all_chunks):\n",
        "        dense_sim = chunk[3] if chunk[3] else 0\n",
        "        sparse_score = bm25_scores[i] if bm25_scores[i] else 0\n",
        "        combined = 0.7 * dense_sim + 0.3 * sparse_score\n",
        "        all_chunks[i] = (*chunk, bm25_scores[i], combined)\n",
        "\n",
        "    all_chunks.sort(key=lambda x: x[5], reverse=True)  # sort by combined score\n",
        "    return all_chunks[:top_k]\n",
        "\n",
        "def query_supabase(user_query, top_k=5):\n",
        "    \"\"\"Hybrid Retrieval (BM25 + Dense Embedding) without reranking.\"\"\"\n",
        "    query_embedding = np.array(get_embedding(user_query), dtype=np.float32).flatten()\n",
        "    keywords = extract_keywords_simple(user_query)\n",
        "    requires_table = query_requires_table(user_query)\n",
        "    query_list = query_embedding.tolist()\n",
        "\n",
        "    conn = psycopg2.connect(DB_CONNECTION)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    ##### TEXT CHUNKS #####\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, 1 - (vec <=> %s) AS similarity\n",
        "        FROM vecs.vec_text\n",
        "        ORDER BY vec <=> %s\n",
        "        LIMIT 10\n",
        "    \"\"\", (json.dumps(query_list), json.dumps(query_list)))\n",
        "    text_chunk_ids = cur.fetchall()\n",
        "\n",
        "    text_results = []\n",
        "    if text_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in text_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, content, metadata\n",
        "            FROM public.documents\n",
        "            WHERE chunk_id IN {chunk_id_list};\n",
        "        \"\"\")\n",
        "        text_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "        text_results = [(cid, \"text\", text_chunks[cid][0], sim) for cid, sim in text_chunk_ids if cid in text_chunks]\n",
        "\n",
        "    ##### TABLE CHUNKS #####\n",
        "    cur.execute(\"\"\"\n",
        "        SELECT id, 1 - (vec <=> %s) AS similarity\n",
        "        FROM vecs.vec_table\n",
        "        ORDER BY vec <=> %s\n",
        "        LIMIT 10\n",
        "    \"\"\", (json.dumps(query_list), json.dumps(query_list)))\n",
        "    table_chunk_ids = cur.fetchall()\n",
        "\n",
        "    table_results = []\n",
        "    if table_chunk_ids:\n",
        "        chunk_id_list = tuple([str(row[0]) for row in table_chunk_ids])\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT chunk_id, description, metadata\n",
        "            FROM public.tables\n",
        "            WHERE chunk_id IN {chunk_id_list};\n",
        "        \"\"\")\n",
        "        table_chunks = {row[0]: row[1:] for row in cur.fetchall()}\n",
        "        table_results = [(cid, \"table\", table_chunks[cid][0], sim) for cid, sim in table_chunk_ids if cid in table_chunks]\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    #### Combine Results and Run Hybrid ####\n",
        "    all_results = text_results + table_results\n",
        "    top_hybrid = hybrid_retrieve(user_query, all_results, top_k=top_k)\n",
        "\n",
        "    return top_hybrid\n"
      ],
      "metadata": {
        "id": "O33eg2hUIPSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM function"
      ],
      "metadata": {
        "id": "U7mvRNuHoj_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# OpenAI API Key\n",
        "OPENAI_API_KEY = \"\"\n",
        "openai.api_key = OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "I2968Quh4YEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pd-BakFwzJD"
      },
      "outputs": [],
      "source": [
        "def call_openai_llm(user_query, retrieved_chunks, chat_history=[]):\n",
        "    \"\"\"Send the query along with retrieved context and chat history to OpenAI API.\"\"\"\n",
        "\n",
        "    # üîπ Sanitize chat history (make sure all entries are dicts)\n",
        "    safe_history = []\n",
        "    for msg in chat_history:\n",
        "        if isinstance(msg, dict) and \"role\" in msg and \"content\" in msg:\n",
        "            safe_history.append(msg)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Skipping malformed chat history entry:\", msg)\n",
        "\n",
        "    # üîπ Prepare context from retrieved chunks\n",
        "    context_text = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk[2]}\" for i, chunk in enumerate(retrieved_chunks)])\n",
        "\n",
        "    # üîπ Construct messages for OpenAI Chat API\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. Use the following retrieved information to answer the user's query.\"},\n",
        "        *safe_history,\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nUser's Question: {user_query}\"}\n",
        "    ]\n",
        "\n",
        "    # üîπ Make API call\n",
        "    client = openai.OpenAI(api_key=openai.api_key)  # New client-style API\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # üîπ Extract and update chat history\n",
        "    answer = response.choices[0].message.content\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "    return answer, chat_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chat history and summarizer as context"
      ],
      "metadata": {
        "id": "cUlDEK3w3KUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "SUMMARIZER_MODEL_ID = \"t5-small\"\n",
        "summarizer_tokenizer = AutoTokenizer.from_pretrained(SUMMARIZER_MODEL_ID)\n",
        "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(SUMMARIZER_MODEL_ID)\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "llm_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text):\n",
        "    return len(llm_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "F0ieq6wN3LNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, max_input_tokens=512, max_output_tokens=150):\n",
        "    inputs = summarizer_tokenizer.encode(\n",
        "        \"summarize: \" + text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_input_tokens,\n",
        "        truncation=True\n",
        "    )\n",
        "    summary_ids = summarizer_model.generate(\n",
        "        inputs,\n",
        "        max_length=max_output_tokens,\n",
        "        min_length=30,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "uzGcnF0J3Ukm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatContextManager:\n",
        "    def __init__(self, summarize_every_turns=3, summarize_every_tokens=1000):\n",
        "        self.chat_history = []\n",
        "        self.summary = \"\"\n",
        "        self.turns_since_last_summary = 0\n",
        "        self.summarize_every_turns = summarize_every_turns\n",
        "        self.summarize_every_tokens = summarize_every_tokens\n",
        "\n",
        "    def summarize_text(self, text, max_input_tokens=512, max_output_tokens=150):\n",
        "        inputs = summarizer_tokenizer.encode(\n",
        "            \"summarize: \" + text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=max_input_tokens,\n",
        "            truncation=True\n",
        "        )\n",
        "        summary_ids = summarizer_model.generate(\n",
        "            inputs,\n",
        "            max_length=max_output_tokens,\n",
        "            min_length=30,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def add_turn(self, user_msg, bot_msg):\n",
        "        self.chat_history.append((\"user\", user_msg))\n",
        "        self.chat_history.append((\"assistant\", bot_msg))\n",
        "        self.turns_since_last_summary += 1\n",
        "\n",
        "        if self.should_summarize():\n",
        "            self.update_summary()\n",
        "\n",
        "    def should_summarize(self):\n",
        "        full_text = \" \".join([msg for _, msg in self.chat_history])\n",
        "        token_count = count_tokens(full_text)\n",
        "        return (\n",
        "            self.turns_since_last_summary >= self.summarize_every_turns\n",
        "            or token_count >= self.summarize_every_tokens\n",
        "        )\n",
        "\n",
        "    def update_summary(self):\n",
        "        full_text = \"\\n\".join([f\"{role}: {msg}\" for role, msg in self.chat_history])\n",
        "        new_summary = self.summarize_text(full_text)\n",
        "        print(\"\\nüìù Summary updated:\\n\", new_summary, \"\\n\")\n",
        "        self.summary = f\"{self.summary}\\n{new_summary}\" if self.summary else new_summary\n",
        "        self.chat_history = []\n",
        "        self.turns_since_last_summary = 0\n",
        "\n",
        "    def get_context_for_llm(self, recent_n=2):\n",
        "        recent = self.chat_history[-recent_n * 2:]\n",
        "        recent_text = \"\\n\".join([f\"{role}: {msg}\" for role, msg in recent])\n",
        "        return f\"Summary:\\n{self.summary}\\n\\nRecent Turns:\\n{recent_text}\""
      ],
      "metadata": {
        "id": "HmisiTAy3YVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST - chat history & summarizer"
      ],
      "metadata": {
        "id": "xvYRFqzWA2rI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before summarizer + chat history\n",
        "\n",
        "def test_llm(user_query):\n",
        "  print(\"\\nüîπüîπüîπüîπüîπüîπ\\n\")\n",
        "  retrieved_chunks = query_supabase(user_query)\n",
        "  print(\"\\nüîπ Input Query:\\n\", user_query)\n",
        "  response, chat_history = call_openai_llm(user_query, retrieved_chunks, [])\n",
        "  print(\"\\nüîπ Chatbot Response:\\n\", response)\n",
        "\n",
        "\n",
        "test_llm(\"what are the uses of AI in the ecosystem?\")\n",
        "test_llm(\"what happens in the US?\")\n",
        "test_llm(\"how is the AI development in the US?\")\n",
        "test_llm(\"give me a company that utilizes this\")\n"
      ],
      "metadata": {
        "id": "F84JWhiHAoTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_ctx = ChatContextManager()\n",
        "\n",
        "print(\"Type '0' to exit, '1' to reset chat history. \\n\")\n",
        "\n",
        "while True:\n",
        "    print(\"\\nüîπüîπüîπüîπüîπüîπ\\n\")\n",
        "    user_query = input(\"\\nüîπ Input Query:\\n\").strip()\n",
        "\n",
        "    if user_query == \"0\":\n",
        "        print(\"\\n exiting...\")\n",
        "        break\n",
        "\n",
        "    if user_query == \"1\":\n",
        "        chat_ctx = ChatContextManager()  # reset context manager\n",
        "        print(\"\\n chat history cleared. \\n\")\n",
        "        continue\n",
        "\n",
        "    retrieved_chunks = query_supabase(user_query)\n",
        "    context = chat_ctx.get_context_for_llm()\n",
        "    chat_history = [{\"role\": \"user\", \"content\": context}] if context.strip() else []\n",
        "\n",
        "    try:\n",
        "        response, _ = call_openai_llm(user_query, retrieved_chunks, chat_history)\n",
        "        print(\"\\n üîπ Chatbot Response:\\n\", response)\n",
        "        chat_ctx.add_turn(user_query, response)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", e)\n"
      ],
      "metadata": {
        "id": "NX5lWkkk-cYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQDzChlpANp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "RyNzm_TH9ILK",
        "OIirlEKR9ENk",
        "PDCvK89wD5Fc",
        "nCdGgRb_D9Xq",
        "cXeLyGEL9AW7",
        "swq_HiH4EyEp",
        "ta6ExA1iNfSl",
        "3f1gtPQOk7E6",
        "OuT9m9Byy-3I",
        "f3WzULiZvnBe",
        "MXpQofBaMf4u",
        "aruyrwyzJ6go",
        "T6K4i3V1ojZ_",
        "U7mvRNuHoj_4",
        "-KxZMPImzXtH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}